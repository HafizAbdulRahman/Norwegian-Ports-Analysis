{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2260307a-7e4e-4b46-b853-919bc469bcb3",
   "metadata": {},
   "source": [
    "# Define Oslo Port polygon and process each file one by one and getting chunck of data like 100000 at once so not to overload the system at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e8107-54f3-4a1c-89e0-cc8d8b361e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Define the working folder (where raw CSV files are located)\n",
    "raw_data_folder = os.getcwd()  # Assumes script is in the same folder as raw data\n",
    "\n",
    "# Create a separate folder for Oslo Port processed files\n",
    "oslo_folder = os.path.join(raw_data_folder, \"processed_oslo_port\")\n",
    "os.makedirs(oslo_folder, exist_ok=True)\n",
    "\n",
    "# Define Oslo Port polygon\n",
    "oslo_polygon = Polygon([\n",
    "    (10.6624659, 59.8957689), (10.7074412, 59.8788027), (10.7706126, 59.898524), \n",
    "    (10.7548197, 59.9150499), (10.6986865, 59.9118659), (10.6624659, 59.8957689)\n",
    "])\n",
    "\n",
    "# Columns to keep for filtering\n",
    "use_columns = [\"date_time_utc\", \"mmsi\", \"longitude\", \"latitude\", \"status\", \n",
    "               \"course_over_ground\", \"speed_over_ground\", \"rate_of_turn\", \n",
    "               \"maneuvre\", \"imo\", \"callsign\", \"ship_name\", \"ship_type\", \n",
    "               \"length\", \"draught\"]\n",
    "\n",
    "# Processing each file one by one\n",
    "for file in sorted(os.listdir(raw_data_folder)):\n",
    "    if file.startswith(\"hais_\") and file.endswith(\".csv\"):  # Process only relevant CSV files\n",
    "        file_path = os.path.join(raw_data_folder, file)\n",
    "        output_path = os.path.join(oslo_folder, file.replace(\".csv\", \"_oslo_filtered.csv\"))\n",
    "\n",
    "        print(f\"üîÑ Processing: {file}...\")\n",
    "\n",
    "        try:\n",
    "            # Initialize row counters\n",
    "            total_rows = 0\n",
    "            filtered_rows = 0\n",
    "\n",
    "            # Process in chunks to avoid memory issues\n",
    "            chunk_size = 100000  \n",
    "            chunks = pd.read_csv(\n",
    "                file_path, \n",
    "                usecols=use_columns, \n",
    "                chunksize=chunk_size, \n",
    "                delimiter=\",\",   # Explicitly set separator\n",
    "                dtype={\"longitude\": float, \"latitude\": float},  # Force correct data types\n",
    "                on_bad_lines=\"warn\",  # Ignore bad lines, log them instead of stopping\n",
    "                encoding_errors=\"ignore\"  # Skip encoding errors\n",
    "            )\n",
    "\n",
    "            for chunk in chunks:\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                # Drop NaN values in important columns\n",
    "                chunk = chunk.dropna(subset=[\"longitude\", \"latitude\"])\n",
    "\n",
    "                # Convert coordinates to geometry\n",
    "                chunk[\"geometry\"] = [Point(xy) for xy in zip(chunk.longitude, chunk.latitude)]\n",
    "                gdf = gpd.GeoDataFrame(chunk, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "                # Filter rows that fall within the Oslo Port polygon\n",
    "                filtered_gdf = gdf[gdf.geometry.within(oslo_polygon)]\n",
    "\n",
    "                if not filtered_gdf.empty:\n",
    "                    filtered_rows += len(filtered_gdf)\n",
    "\n",
    "                    # Save filtered data incrementally\n",
    "                    filtered_gdf.drop(columns=[\"geometry\"]).to_csv(output_path, mode=\"a\", index=False)\n",
    "\n",
    "            print(f\"‚úî Completed: {file} | Total Rows: {total_rows} | Kept: {filtered_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All files processed for Oslo Port! Check the 'processed_oslo_port' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538b5dc9-203e-47f5-8eac-e05eff00df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Re-processing: hais_2024-01-05.csv...\n",
      "‚úî Completed: hais_2024-01-05.csv | Total Rows: 1340991 | Kept: 8761\n",
      "üîÑ Re-processing: hais_2024-02-13.csv...\n",
      "‚úî Completed: hais_2024-02-13.csv | Total Rows: 1293102 | Kept: 6441\n",
      "üîÑ Re-processing: hais_2024-02-14.csv...\n",
      "‚úî Completed: hais_2024-02-14.csv | Total Rows: 1275139 | Kept: 7096\n",
      "üîÑ Re-processing: hais_2024-02-15.csv...\n",
      "‚úî Completed: hais_2024-02-15.csv | Total Rows: 1350220 | Kept: 9271\n",
      "üîÑ Re-processing: hais_2024-02-16.csv...\n",
      "‚úî Completed: hais_2024-02-16.csv | Total Rows: 1355142 | Kept: 8852\n",
      "üîÑ Re-processing: hais_2024-02-19.csv...\n",
      "‚úî Completed: hais_2024-02-19.csv | Total Rows: 1152611 | Kept: 5291\n",
      "üîÑ Re-processing: hais_2024-02-22.csv...\n",
      "‚úî Completed: hais_2024-02-22.csv | Total Rows: 1364070 | Kept: 2150\n",
      "üîÑ Re-processing: hais_2024-04-03.csv...\n",
      "‚úî Completed: hais_2024-04-03.csv | Total Rows: 1145778 | Kept: 2394\n",
      "üîÑ Re-processing: hais_2024-04-04.csv...\n",
      "‚úî Completed: hais_2024-04-04.csv | Total Rows: 1261808 | Kept: 7046\n",
      "üîÑ Re-processing: hais_2024-04-05.csv...\n",
      "‚úî Completed: hais_2024-04-05.csv | Total Rows: 1337500 | Kept: 9323\n",
      "üîÑ Re-processing: hais_2024-04-26.csv...\n",
      "‚úî Completed: hais_2024-04-26.csv | Total Rows: 1196245 | Kept: 6352\n",
      "üîÑ Re-processing: hais_2024-04-30.csv...\n",
      "‚úî Completed: hais_2024-04-30.csv | Total Rows: 1358923 | Kept: 8709\n",
      "\n",
      "‚úÖ All remaining problematic files processed! Check the 'remaining_processed_oslo_files' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import csv\n",
    "\n",
    "# Define the working folder (where raw CSV files are located)\n",
    "raw_data_folder = os.getcwd()  # Assumes script is in the same folder as raw data\n",
    "\n",
    "# Create a separate folder for remaining processed Oslo files\n",
    "remaining_oslo_folder = os.path.join(raw_data_folder, \"remaining_processed_oslo_files\")\n",
    "os.makedirs(remaining_oslo_folder, exist_ok=True)\n",
    "\n",
    "# Define Oslo Port polygon\n",
    "oslo_polygon = Polygon([\n",
    "    (10.6624659, 59.8957689), (10.7074412, 59.8788027), (10.7706126, 59.898524), \n",
    "    (10.7548197, 59.9150499), (10.6986865, 59.9118659), (10.6624659, 59.8957689)\n",
    "])\n",
    "\n",
    "# List of problematic files to process\n",
    "problematic_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "# Columns to keep for filtering\n",
    "use_columns = [\"date_time_utc\", \"mmsi\", \"longitude\", \"latitude\", \"status\", \n",
    "               \"course_over_ground\", \"speed_over_ground\", \"rate_of_turn\", \n",
    "               \"maneuvre\", \"imo\", \"callsign\", \"ship_name\", \"ship_type\", \n",
    "               \"length\", \"draught\"]\n",
    "\n",
    "# Processing each problematic file\n",
    "for file in problematic_files:\n",
    "    file_path = os.path.join(raw_data_folder, file)\n",
    "    output_path = os.path.join(remaining_oslo_folder, file.replace(\".csv\", \"_oslo_filtered.csv\"))\n",
    "\n",
    "    if os.path.exists(file_path):  # Ensure file exists\n",
    "        print(f\"üîÑ Re-processing: {file}...\")\n",
    "\n",
    "        try:\n",
    "            total_rows = 0\n",
    "            filtered_rows = 0\n",
    "\n",
    "            # Process in chunks to avoid memory issues\n",
    "            chunk_size = 100000  \n",
    "            chunks = pd.read_csv(\n",
    "                file_path, \n",
    "                usecols=use_columns, \n",
    "                chunksize=chunk_size, \n",
    "                delimiter=\",\",  # Explicit separator\n",
    "                dtype={\"longitude\": float, \"latitude\": float},  # Force correct data types\n",
    "                on_bad_lines=\"warn\",  # Ignore and warn about bad lines\n",
    "                encoding_errors=\"ignore\",  # Ignore encoding errors\n",
    "                quoting=csv.QUOTE_NONE  # Treat quotes as regular characters\n",
    "            )\n",
    "\n",
    "            for chunk in chunks:\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                # Drop NaN values in critical columns\n",
    "                chunk = chunk.dropna(subset=[\"longitude\", \"latitude\"])\n",
    "\n",
    "                # Convert coordinates to geometry\n",
    "                chunk[\"geometry\"] = [Point(xy) for xy in zip(chunk.longitude, chunk.latitude)]\n",
    "                gdf = gpd.GeoDataFrame(chunk, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "                # Filter rows that fall within the Oslo Port polygon\n",
    "                filtered_gdf = gdf[gdf.geometry.within(oslo_polygon)]\n",
    "\n",
    "                if not filtered_gdf.empty:\n",
    "                    filtered_rows += len(filtered_gdf)\n",
    "\n",
    "                    # Save filtered data incrementally\n",
    "                    filtered_gdf.drop(columns=[\"geometry\"]).to_csv(output_path, mode=\"a\", index=False)\n",
    "\n",
    "            print(f\"‚úî Completed: {file} | Total Rows: {total_rows} | Kept: {filtered_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All remaining problematic files processed! Check the 'remaining_processed_oslo_files' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb2f25-8391-43bf-95e0-e2f7e2d55513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
