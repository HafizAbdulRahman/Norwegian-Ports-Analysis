{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb054034-4fa3-4c02-bd9e-77d865177742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: hais_2024-01-01.csv...\n",
      "✔ Completed: hais_2024-01-01.csv | Total Rows: 1264740 | Kept: 8730\n",
      "Processing: hais_2024-01-01_compressed.csv...\n",
      "✔ Completed: hais_2024-01-01_compressed.csv | Total Rows: 64069 | Kept: 8\n",
      "Processing: hais_2024-01-02.csv...\n",
      "✔ Completed: hais_2024-01-02.csv | Total Rows: 1168608 | Kept: 8935\n",
      "Processing: hais_2024-01-03.csv...\n",
      "✔ Completed: hais_2024-01-03.csv | Total Rows: 1213077 | Kept: 4113\n",
      "Processing: hais_2024-01-04.csv...\n",
      "✔ Completed: hais_2024-01-04.csv | Total Rows: 1294664 | Kept: 4936\n",
      "Processing: hais_2024-01-05.csv...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 175732",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m  \u001b[38;5;66;03m# Adjust based on system memory\u001b[39;00m\n\u001b[0;32m     39\u001b[0m chunks \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, usecols\u001b[38;5;241m=\u001b[39muse_columns, chunksize\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m     42\u001b[0m     total_rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Convert coordinates to geometry\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chunk()\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nrows\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 175732"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Define the working folder (where raw CSV files are located)\n",
    "raw_data_folder = os.getcwd()  # Assumes script is in the same folder as raw data\n",
    "\n",
    "# Create a separate folder for Oslo Port processed files\n",
    "oslo_folder = os.path.join(raw_data_folder, \"processed_oslo_port\")\n",
    "os.makedirs(oslo_folder, exist_ok=True)\n",
    "\n",
    "# Define Oslo Port polygon\n",
    "oslo_polygon = Polygon([\n",
    "    (10.6624659, 59.8957689), (10.7074412, 59.8788027), (10.7706126, 59.898524), \n",
    "    (10.7548197, 59.9150499), (10.6986865, 59.9118659), (10.6624659, 59.8957689)\n",
    "])\n",
    "\n",
    "# Columns to keep for filtering (no need to load unnecessary data)\n",
    "use_columns = [\"date_time_utc\", \"mmsi\", \"longitude\", \"latitude\", \"status\", \n",
    "               \"course_over_ground\", \"speed_over_ground\", \"rate_of_turn\", \n",
    "               \"maneuvre\", \"imo\", \"callsign\", \"ship_name\", \"ship_type\", \n",
    "               \"length\", \"draught\"]\n",
    "\n",
    "# Processing each file one by one\n",
    "for file in sorted(os.listdir(raw_data_folder)):\n",
    "    if file.startswith(\"hais_\") and file.endswith(\".csv\"):  # Process only relevant CSV files\n",
    "        file_path = os.path.join(raw_data_folder, file)\n",
    "        output_path = os.path.join(oslo_folder, file.replace(\".csv\", \"_oslo_filtered.csv\"))\n",
    "\n",
    "        print(f\"Processing: {file}...\")\n",
    "\n",
    "        # Initialize row counters\n",
    "        total_rows = 0\n",
    "        filtered_rows = 0\n",
    "\n",
    "        # Process in chunks to avoid memory issues\n",
    "        chunk_size = 100000  # Adjust based on system memory\n",
    "        chunks = pd.read_csv(file_path, usecols=use_columns, chunksize=chunk_size)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "            # Convert coordinates to geometry\n",
    "            chunk[\"geometry\"] = [Point(xy) for xy in zip(chunk.longitude, chunk.latitude)]\n",
    "            gdf = gpd.GeoDataFrame(chunk, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "            # Filter rows that fall within the Oslo Port polygon\n",
    "            filtered_gdf = gdf[gdf.geometry.within(oslo_polygon)]\n",
    "\n",
    "            if not filtered_gdf.empty:\n",
    "                filtered_rows += len(filtered_gdf)\n",
    "\n",
    "                # Save filtered data incrementally\n",
    "                filtered_gdf.drop(columns=[\"geometry\"]).to_csv(output_path, mode=\"a\", index=False)\n",
    "\n",
    "        print(f\"✔ Completed: {file} | Total Rows: {total_rows} | Kept: {filtered_rows}\")\n",
    "\n",
    "print(\"\\n✅ All files processed for Oslo Port! Check the 'processed_oslo_port' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daee27c0-4da0-4a85-84b0-3713d18fb5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing: hais_2024-01-01.csv...\n",
      "✔ Completed: hais_2024-01-01.csv | Total Rows: 1264740 | Kept: 8730\n",
      "🔄 Processing: hais_2024-01-01_compressed.csv...\n",
      "✔ Completed: hais_2024-01-01_compressed.csv | Total Rows: 64069 | Kept: 8\n",
      "🔄 Processing: hais_2024-01-02.csv...\n",
      "✔ Completed: hais_2024-01-02.csv | Total Rows: 1168608 | Kept: 8935\n",
      "🔄 Processing: hais_2024-01-03.csv...\n",
      "✔ Completed: hais_2024-01-03.csv | Total Rows: 1213077 | Kept: 4113\n",
      "🔄 Processing: hais_2024-01-04.csv...\n",
      "✔ Completed: hais_2024-01-04.csv | Total Rows: 1294664 | Kept: 4936\n",
      "🔄 Processing: hais_2024-01-05.csv...\n",
      "❌ Error processing hais_2024-01-05.csv: Error tokenizing data. C error: EOF inside string starting at row 175732\n",
      "🔄 Processing: hais_2024-01-06.csv...\n",
      "✔ Completed: hais_2024-01-06.csv | Total Rows: 1221781 | Kept: 9160\n",
      "🔄 Processing: hais_2024-01-07.csv...\n",
      "✔ Completed: hais_2024-01-07.csv | Total Rows: 1123060 | Kept: 9134\n",
      "🔄 Processing: hais_2024-01-08.csv...\n",
      "✔ Completed: hais_2024-01-08.csv | Total Rows: 1208519 | Kept: 10086\n",
      "🔄 Processing: hais_2024-01-09.csv...\n",
      "✔ Completed: hais_2024-01-09.csv | Total Rows: 1198969 | Kept: 9627\n",
      "🔄 Processing: hais_2024-01-10.csv...\n",
      "✔ Completed: hais_2024-01-10.csv | Total Rows: 1238576 | Kept: 8763\n",
      "🔄 Processing: hais_2024-01-11.csv...\n",
      "✔ Completed: hais_2024-01-11.csv | Total Rows: 1295277 | Kept: 8783\n",
      "🔄 Processing: hais_2024-01-12.csv...\n",
      "✔ Completed: hais_2024-01-12.csv | Total Rows: 1317674 | Kept: 4896\n",
      "🔄 Processing: hais_2024-01-13.csv...\n",
      "✔ Completed: hais_2024-01-13.csv | Total Rows: 1455110 | Kept: 453\n",
      "🔄 Processing: hais_2024-01-14.csv...\n",
      "✔ Completed: hais_2024-01-14.csv | Total Rows: 1351421 | Kept: 677\n",
      "🔄 Processing: hais_2024-01-15.csv...\n",
      "✔ Completed: hais_2024-01-15.csv | Total Rows: 1400088 | Kept: 737\n",
      "🔄 Processing: hais_2024-01-16.csv...\n",
      "✔ Completed: hais_2024-01-16.csv | Total Rows: 1190779 | Kept: 668\n",
      "🔄 Processing: hais_2024-01-17.csv...\n",
      "✔ Completed: hais_2024-01-17.csv | Total Rows: 1253710 | Kept: 5831\n",
      "🔄 Processing: hais_2024-01-18.csv...\n",
      "✔ Completed: hais_2024-01-18.csv | Total Rows: 1377242 | Kept: 9032\n",
      "🔄 Processing: hais_2024-01-19.csv...\n",
      "✔ Completed: hais_2024-01-19.csv | Total Rows: 1412424 | Kept: 9027\n",
      "🔄 Processing: hais_2024-01-20.csv...\n",
      "✔ Completed: hais_2024-01-20.csv | Total Rows: 1424547 | Kept: 9040\n",
      "🔄 Processing: hais_2024-01-21.csv...\n",
      "✔ Completed: hais_2024-01-21.csv | Total Rows: 1276551 | Kept: 9100\n",
      "🔄 Processing: hais_2024-01-22.csv...\n",
      "✔ Completed: hais_2024-01-22.csv | Total Rows: 1250247 | Kept: 9033\n",
      "🔄 Processing: hais_2024-01-23.csv...\n",
      "✔ Completed: hais_2024-01-23.csv | Total Rows: 1221990 | Kept: 9008\n",
      "🔄 Processing: hais_2024-01-24.csv...\n",
      "✔ Completed: hais_2024-01-24.csv | Total Rows: 1230476 | Kept: 9192\n",
      "🔄 Processing: hais_2024-01-25.csv...\n",
      "✔ Completed: hais_2024-01-25.csv | Total Rows: 1288063 | Kept: 8986\n",
      "🔄 Processing: hais_2024-01-26.csv...\n",
      "✔ Completed: hais_2024-01-26.csv | Total Rows: 1274393 | Kept: 9329\n",
      "🔄 Processing: hais_2024-01-27.csv...\n",
      "✔ Completed: hais_2024-01-27.csv | Total Rows: 1381276 | Kept: 9510\n",
      "🔄 Processing: hais_2024-01-28.csv...\n",
      "✔ Completed: hais_2024-01-28.csv | Total Rows: 1319085 | Kept: 8657\n",
      "🔄 Processing: hais_2024-01-29.csv...\n",
      "✔ Completed: hais_2024-01-29.csv | Total Rows: 1294987 | Kept: 11299\n",
      "🔄 Processing: hais_2024-01-30.csv...\n",
      "✔ Completed: hais_2024-01-30.csv | Total Rows: 1155041 | Kept: 7957\n",
      "🔄 Processing: hais_2024-01-31.csv...\n",
      "✔ Completed: hais_2024-01-31.csv | Total Rows: 1268139 | Kept: 3423\n",
      "🔄 Processing: hais_2024-02-01.csv...\n",
      "✔ Completed: hais_2024-02-01.csv | Total Rows: 1154987 | Kept: 1634\n",
      "🔄 Processing: hais_2024-02-02.csv...\n",
      "✔ Completed: hais_2024-02-02.csv | Total Rows: 1379159 | Kept: 2369\n",
      "🔄 Processing: hais_2024-02-03.csv...\n",
      "✔ Completed: hais_2024-02-03.csv | Total Rows: 1241743 | Kept: 471\n",
      "🔄 Processing: hais_2024-02-04.csv...\n",
      "✔ Completed: hais_2024-02-04.csv | Total Rows: 1200714 | Kept: 596\n",
      "🔄 Processing: hais_2024-02-05.csv...\n",
      "✔ Completed: hais_2024-02-05.csv | Total Rows: 1332405 | Kept: 1639\n",
      "🔄 Processing: hais_2024-02-06.csv...\n",
      "✔ Completed: hais_2024-02-06.csv | Total Rows: 1509765 | Kept: 1659\n",
      "🔄 Processing: hais_2024-02-07.csv...\n",
      "✔ Completed: hais_2024-02-07.csv | Total Rows: 1489558 | Kept: 2001\n",
      "🔄 Processing: hais_2024-02-08.csv...\n",
      "✔ Completed: hais_2024-02-08.csv | Total Rows: 1310775 | Kept: 2745\n",
      "🔄 Processing: hais_2024-02-09.csv...\n",
      "✔ Completed: hais_2024-02-09.csv | Total Rows: 1259947 | Kept: 1986\n",
      "🔄 Processing: hais_2024-02-10.csv...\n",
      "✔ Completed: hais_2024-02-10.csv | Total Rows: 1280999 | Kept: 979\n",
      "🔄 Processing: hais_2024-02-11.csv...\n",
      "✔ Completed: hais_2024-02-11.csv | Total Rows: 1383506 | Kept: 966\n",
      "🔄 Processing: hais_2024-02-12.csv...\n",
      "✔ Completed: hais_2024-02-12.csv | Total Rows: 1407907 | Kept: 1318\n",
      "🔄 Processing: hais_2024-02-13.csv...\n",
      "❌ Error processing hais_2024-02-13.csv: Error tokenizing data. C error: EOF inside string starting at row 1085892\n",
      "🔄 Processing: hais_2024-02-14.csv...\n",
      "❌ Error processing hais_2024-02-14.csv: Error tokenizing data. C error: EOF inside string starting at row 738231\n",
      "🔄 Processing: hais_2024-02-15.csv...\n",
      "❌ Error processing hais_2024-02-15.csv: Error tokenizing data. C error: EOF inside string starting at row 1341694\n",
      "🔄 Processing: hais_2024-02-16.csv...\n",
      "❌ Error processing hais_2024-02-16.csv: Error tokenizing data. C error: EOF inside string starting at row 116773\n",
      "🔄 Processing: hais_2024-02-17.csv...\n",
      "✔ Completed: hais_2024-02-17.csv | Total Rows: 1318988 | Kept: 8601\n",
      "🔄 Processing: hais_2024-02-18.csv...\n",
      "✔ Completed: hais_2024-02-18.csv | Total Rows: 1233601 | Kept: 8631\n",
      "🔄 Processing: hais_2024-02-19.csv...\n",
      "❌ Error processing hais_2024-02-19.csv: Error tokenizing data. C error: EOF inside string starting at row 475130\n",
      "🔄 Processing: hais_2024-02-20.csv...\n",
      "✔ Completed: hais_2024-02-20.csv | Total Rows: 1260164 | Kept: 1316\n",
      "🔄 Processing: hais_2024-02-21.csv...\n",
      "✔ Completed: hais_2024-02-21.csv | Total Rows: 1331946 | Kept: 2048\n",
      "🔄 Processing: hais_2024-02-22.csv...\n",
      "❌ Error processing hais_2024-02-22.csv: Error tokenizing data. C error: EOF inside string starting at row 388656\n",
      "🔄 Processing: hais_2024-02-23.csv...\n",
      "✔ Completed: hais_2024-02-23.csv | Total Rows: 1270274 | Kept: 1480\n",
      "🔄 Processing: hais_2024-02-24.csv...\n",
      "✔ Completed: hais_2024-02-24.csv | Total Rows: 1357254 | Kept: 1302\n",
      "🔄 Processing: hais_2024-02-25.csv...\n",
      "✔ Completed: hais_2024-02-25.csv | Total Rows: 1249375 | Kept: 1456\n",
      "🔄 Processing: hais_2024-02-26.csv...\n",
      "✔ Completed: hais_2024-02-26.csv | Total Rows: 1272398 | Kept: 1917\n",
      "🔄 Processing: hais_2024-02-27.csv...\n",
      "✔ Completed: hais_2024-02-27.csv | Total Rows: 1277558 | Kept: 2678\n",
      "🔄 Processing: hais_2024-02-28.csv...\n",
      "✔ Completed: hais_2024-02-28.csv | Total Rows: 1379308 | Kept: 1141\n",
      "🔄 Processing: hais_2024-02-29.csv...\n",
      "✔ Completed: hais_2024-02-29.csv | Total Rows: 1352344 | Kept: 1971\n",
      "🔄 Processing: hais_2024-03-01.csv...\n",
      "✔ Completed: hais_2024-03-01.csv | Total Rows: 1387201 | Kept: 1774\n",
      "🔄 Processing: hais_2024-03-02.csv...\n",
      "✔ Completed: hais_2024-03-02.csv | Total Rows: 1289265 | Kept: 881\n",
      "🔄 Processing: hais_2024-03-03.csv...\n",
      "✔ Completed: hais_2024-03-03.csv | Total Rows: 1320387 | Kept: 885\n",
      "🔄 Processing: hais_2024-03-04.csv...\n",
      "✔ Completed: hais_2024-03-04.csv | Total Rows: 1216746 | Kept: 7438\n",
      "🔄 Processing: hais_2024-03-05.csv...\n",
      "✔ Completed: hais_2024-03-05.csv | Total Rows: 1195013 | Kept: 7699\n",
      "🔄 Processing: hais_2024-03-06.csv...\n",
      "✔ Completed: hais_2024-03-06.csv | Total Rows: 1253064 | Kept: 10098\n",
      "🔄 Processing: hais_2024-03-07.csv...\n",
      "✔ Completed: hais_2024-03-07.csv | Total Rows: 1341241 | Kept: 9099\n",
      "🔄 Processing: hais_2024-03-08.csv...\n",
      "✔ Completed: hais_2024-03-08.csv | Total Rows: 1298136 | Kept: 9242\n",
      "🔄 Processing: hais_2024-03-09.csv...\n",
      "✔ Completed: hais_2024-03-09.csv | Total Rows: 1286515 | Kept: 8759\n",
      "🔄 Processing: hais_2024-03-10.csv...\n",
      "✔ Completed: hais_2024-03-10.csv | Total Rows: 1271310 | Kept: 8635\n",
      "🔄 Processing: hais_2024-03-11.csv...\n",
      "✔ Completed: hais_2024-03-11.csv | Total Rows: 1329780 | Kept: 3973\n",
      "🔄 Processing: hais_2024-03-12.csv...\n",
      "✔ Completed: hais_2024-03-12.csv | Total Rows: 1320939 | Kept: 937\n",
      "🔄 Processing: hais_2024-03-13.csv...\n",
      "✔ Completed: hais_2024-03-13.csv | Total Rows: 1291325 | Kept: 1777\n",
      "🔄 Processing: hais_2024-03-14.csv...\n",
      "✔ Completed: hais_2024-03-14.csv | Total Rows: 1400885 | Kept: 1501\n",
      "🔄 Processing: hais_2024-03-15.csv...\n",
      "✔ Completed: hais_2024-03-15.csv | Total Rows: 1341845 | Kept: 683\n",
      "🔄 Processing: hais_2024-03-16.csv...\n",
      "✔ Completed: hais_2024-03-16.csv | Total Rows: 1397032 | Kept: 475\n",
      "🔄 Processing: hais_2024-03-17.csv...\n",
      "✔ Completed: hais_2024-03-17.csv | Total Rows: 1373076 | Kept: 957\n",
      "🔄 Processing: hais_2024-03-18.csv...\n",
      "✔ Completed: hais_2024-03-18.csv | Total Rows: 1355590 | Kept: 1385\n",
      "🔄 Processing: hais_2024-03-19.csv...\n",
      "✔ Completed: hais_2024-03-19.csv | Total Rows: 1364817 | Kept: 2240\n",
      "🔄 Processing: hais_2024-03-20.csv...\n",
      "✔ Completed: hais_2024-03-20.csv | Total Rows: 1378222 | Kept: 1638\n",
      "🔄 Processing: hais_2024-03-21.csv...\n",
      "✔ Completed: hais_2024-03-21.csv | Total Rows: 1385193 | Kept: 1325\n",
      "🔄 Processing: hais_2024-03-22.csv...\n",
      "✔ Completed: hais_2024-03-22.csv | Total Rows: 1502548 | Kept: 1183\n",
      "🔄 Processing: hais_2024-03-23.csv...\n",
      "✔ Completed: hais_2024-03-23.csv | Total Rows: 1261446 | Kept: 1106\n",
      "🔄 Processing: hais_2024-03-24.csv...\n",
      "✔ Completed: hais_2024-03-24.csv | Total Rows: 1343383 | Kept: 743\n",
      "🔄 Processing: hais_2024-03-25.csv...\n",
      "✔ Completed: hais_2024-03-25.csv | Total Rows: 1244512 | Kept: 1072\n",
      "🔄 Processing: hais_2024-03-26.csv...\n",
      "✔ Completed: hais_2024-03-26.csv | Total Rows: 1233072 | Kept: 1148\n",
      "🔄 Processing: hais_2024-03-27.csv...\n",
      "✔ Completed: hais_2024-03-27.csv | Total Rows: 1256730 | Kept: 849\n",
      "🔄 Processing: hais_2024-03-28.csv...\n",
      "✔ Completed: hais_2024-03-28.csv | Total Rows: 1185204 | Kept: 536\n",
      "🔄 Processing: hais_2024-03-29.csv...\n",
      "✔ Completed: hais_2024-03-29.csv | Total Rows: 1246143 | Kept: 556\n",
      "🔄 Processing: hais_2024-03-30.csv...\n",
      "✔ Completed: hais_2024-03-30.csv | Total Rows: 1204529 | Kept: 1470\n",
      "🔄 Processing: hais_2024-03-31.csv...\n",
      "✔ Completed: hais_2024-03-31.csv | Total Rows: 1127011 | Kept: 1486\n",
      "🔄 Processing: hais_2024-04-01.csv...\n",
      "✔ Completed: hais_2024-04-01.csv | Total Rows: 1146842 | Kept: 1419\n",
      "🔄 Processing: hais_2024-04-02.csv...\n",
      "✔ Completed: hais_2024-04-02.csv | Total Rows: 1209959 | Kept: 2495\n",
      "🔄 Processing: hais_2024-04-03.csv...\n",
      "❌ Error processing hais_2024-04-03.csv: Error tokenizing data. C error: EOF inside string starting at row 508157\n",
      "🔄 Processing: hais_2024-04-04.csv...\n",
      "❌ Error processing hais_2024-04-04.csv: Error tokenizing data. C error: EOF inside string starting at row 992934\n",
      "🔄 Processing: hais_2024-04-05.csv...\n",
      "❌ Error processing hais_2024-04-05.csv: Error tokenizing data. C error: EOF inside string starting at row 565686\n",
      "🔄 Processing: hais_2024-04-06.csv...\n",
      "✔ Completed: hais_2024-04-06.csv | Total Rows: 1411938 | Kept: 10548\n",
      "🔄 Processing: hais_2024-04-07.csv...\n",
      "✔ Completed: hais_2024-04-07.csv | Total Rows: 1347505 | Kept: 9882\n",
      "🔄 Processing: hais_2024-04-08.csv...\n",
      "✔ Completed: hais_2024-04-08.csv | Total Rows: 1305943 | Kept: 4177\n",
      "🔄 Processing: hais_2024-04-09.csv...\n",
      "✔ Completed: hais_2024-04-09.csv | Total Rows: 1232212 | Kept: 2847\n",
      "🔄 Processing: hais_2024-04-10.csv...\n",
      "✔ Completed: hais_2024-04-10.csv | Total Rows: 1271994 | Kept: 2837\n",
      "🔄 Processing: hais_2024-04-11.csv...\n",
      "✔ Completed: hais_2024-04-11.csv | Total Rows: 1330889 | Kept: 1917\n",
      "🔄 Processing: hais_2024-04-12.csv...\n",
      "✔ Completed: hais_2024-04-12.csv | Total Rows: 1403230 | Kept: 1539\n",
      "🔄 Processing: hais_2024-04-13.csv...\n",
      "✔ Completed: hais_2024-04-13.csv | Total Rows: 1311177 | Kept: 475\n",
      "🔄 Processing: hais_2024-04-14.csv...\n",
      "✔ Completed: hais_2024-04-14.csv | Total Rows: 1285723 | Kept: 471\n",
      "🔄 Processing: hais_2024-04-15.csv...\n",
      "✔ Completed: hais_2024-04-15.csv | Total Rows: 1303793 | Kept: 5143\n",
      "🔄 Processing: hais_2024-04-16.csv...\n",
      "✔ Completed: hais_2024-04-16.csv | Total Rows: 1307958 | Kept: 4259\n",
      "🔄 Processing: hais_2024-04-17.csv...\n",
      "✔ Completed: hais_2024-04-17.csv | Total Rows: 1361375 | Kept: 939\n",
      "🔄 Processing: hais_2024-04-18.csv...\n",
      "✔ Completed: hais_2024-04-18.csv | Total Rows: 1416324 | Kept: 1722\n",
      "🔄 Processing: hais_2024-04-19.csv...\n",
      "✔ Completed: hais_2024-04-19.csv | Total Rows: 1403314 | Kept: 7868\n",
      "🔄 Processing: hais_2024-04-20.csv...\n",
      "✔ Completed: hais_2024-04-20.csv | Total Rows: 1251251 | Kept: 8604\n",
      "🔄 Processing: hais_2024-04-21.csv...\n",
      "✔ Completed: hais_2024-04-21.csv | Total Rows: 1148826 | Kept: 8626\n",
      "🔄 Processing: hais_2024-04-22.csv...\n",
      "✔ Completed: hais_2024-04-22.csv | Total Rows: 1241963 | Kept: 9691\n",
      "🔄 Processing: hais_2024-04-23.csv...\n",
      "✔ Completed: hais_2024-04-23.csv | Total Rows: 1262453 | Kept: 8997\n",
      "🔄 Processing: hais_2024-04-24.csv...\n",
      "✔ Completed: hais_2024-04-24.csv | Total Rows: 1320357 | Kept: 7990\n",
      "🔄 Processing: hais_2024-04-25.csv...\n",
      "✔ Completed: hais_2024-04-25.csv | Total Rows: 1260368 | Kept: 8951\n",
      "🔄 Processing: hais_2024-04-26.csv...\n",
      "❌ Error processing hais_2024-04-26.csv: Error tokenizing data. C error: EOF inside string starting at row 1155567\n",
      "🔄 Processing: hais_2024-04-27.csv...\n",
      "✔ Completed: hais_2024-04-27.csv | Total Rows: 1290112 | Kept: 9281\n",
      "🔄 Processing: hais_2024-04-28.csv...\n",
      "✔ Completed: hais_2024-04-28.csv | Total Rows: 1366811 | Kept: 9101\n",
      "🔄 Processing: hais_2024-04-29.csv...\n",
      "✔ Completed: hais_2024-04-29.csv | Total Rows: 1438768 | Kept: 10365\n",
      "🔄 Processing: hais_2024-04-30.csv...\n",
      "❌ Error processing hais_2024-04-30.csv: Error tokenizing data. C error: EOF inside string starting at row 574898\n",
      "\n",
      "✅ All files processed for Oslo Port! Check the 'processed_oslo_port' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Define the working folder (where raw CSV files are located)\n",
    "raw_data_folder = os.getcwd()  # Assumes script is in the same folder as raw data\n",
    "\n",
    "# Create a separate folder for Oslo Port processed files\n",
    "oslo_folder = os.path.join(raw_data_folder, \"processed_oslo_port\")\n",
    "os.makedirs(oslo_folder, exist_ok=True)\n",
    "\n",
    "# Define Oslo Port polygon\n",
    "oslo_polygon = Polygon([\n",
    "    (10.6624659, 59.8957689), (10.7074412, 59.8788027), (10.7706126, 59.898524), \n",
    "    (10.7548197, 59.9150499), (10.6986865, 59.9118659), (10.6624659, 59.8957689)\n",
    "])\n",
    "\n",
    "# Columns to keep for filtering\n",
    "use_columns = [\"date_time_utc\", \"mmsi\", \"longitude\", \"latitude\", \"status\", \n",
    "               \"course_over_ground\", \"speed_over_ground\", \"rate_of_turn\", \n",
    "               \"maneuvre\", \"imo\", \"callsign\", \"ship_name\", \"ship_type\", \n",
    "               \"length\", \"draught\"]\n",
    "\n",
    "# Processing each file one by one\n",
    "for file in sorted(os.listdir(raw_data_folder)):\n",
    "    if file.startswith(\"hais_\") and file.endswith(\".csv\"):  # Process only relevant CSV files\n",
    "        file_path = os.path.join(raw_data_folder, file)\n",
    "        output_path = os.path.join(oslo_folder, file.replace(\".csv\", \"_oslo_filtered.csv\"))\n",
    "\n",
    "        print(f\"🔄 Processing: {file}...\")\n",
    "\n",
    "        try:\n",
    "            # Initialize row counters\n",
    "            total_rows = 0\n",
    "            filtered_rows = 0\n",
    "\n",
    "            # Process in chunks to avoid memory issues\n",
    "            chunk_size = 100000  \n",
    "            chunks = pd.read_csv(\n",
    "                file_path, \n",
    "                usecols=use_columns, \n",
    "                chunksize=chunk_size, \n",
    "                delimiter=\",\",   # Explicitly set separator\n",
    "                dtype={\"longitude\": float, \"latitude\": float},  # Force correct data types\n",
    "                on_bad_lines=\"warn\",  # Ignore bad lines, log them instead of stopping\n",
    "                encoding_errors=\"ignore\"  # Skip encoding errors\n",
    "            )\n",
    "\n",
    "            for chunk in chunks:\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                # Drop NaN values in important columns\n",
    "                chunk = chunk.dropna(subset=[\"longitude\", \"latitude\"])\n",
    "\n",
    "                # Convert coordinates to geometry\n",
    "                chunk[\"geometry\"] = [Point(xy) for xy in zip(chunk.longitude, chunk.latitude)]\n",
    "                gdf = gpd.GeoDataFrame(chunk, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "                # Filter rows that fall within the Oslo Port polygon\n",
    "                filtered_gdf = gdf[gdf.geometry.within(oslo_polygon)]\n",
    "\n",
    "                if not filtered_gdf.empty:\n",
    "                    filtered_rows += len(filtered_gdf)\n",
    "\n",
    "                    # Save filtered data incrementally\n",
    "                    filtered_gdf.drop(columns=[\"geometry\"]).to_csv(output_path, mode=\"a\", index=False)\n",
    "\n",
    "            print(f\"✔ Completed: {file} | Total Rows: {total_rows} | Kept: {filtered_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "print(\"\\n✅ All files processed for Oslo Port! Check the 'processed_oslo_port' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538b5dc9-203e-47f5-8eac-e05eff00df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Re-processing: hais_2024-01-05.csv...\n",
      "✔ Completed: hais_2024-01-05.csv | Total Rows: 1340991 | Kept: 8761\n",
      "🔄 Re-processing: hais_2024-02-13.csv...\n",
      "✔ Completed: hais_2024-02-13.csv | Total Rows: 1293102 | Kept: 6441\n",
      "🔄 Re-processing: hais_2024-02-14.csv...\n",
      "✔ Completed: hais_2024-02-14.csv | Total Rows: 1275139 | Kept: 7096\n",
      "🔄 Re-processing: hais_2024-02-15.csv...\n",
      "✔ Completed: hais_2024-02-15.csv | Total Rows: 1350220 | Kept: 9271\n",
      "🔄 Re-processing: hais_2024-02-16.csv...\n",
      "✔ Completed: hais_2024-02-16.csv | Total Rows: 1355142 | Kept: 8852\n",
      "🔄 Re-processing: hais_2024-02-19.csv...\n",
      "✔ Completed: hais_2024-02-19.csv | Total Rows: 1152611 | Kept: 5291\n",
      "🔄 Re-processing: hais_2024-02-22.csv...\n",
      "✔ Completed: hais_2024-02-22.csv | Total Rows: 1364070 | Kept: 2150\n",
      "🔄 Re-processing: hais_2024-04-03.csv...\n",
      "✔ Completed: hais_2024-04-03.csv | Total Rows: 1145778 | Kept: 2394\n",
      "🔄 Re-processing: hais_2024-04-04.csv...\n",
      "✔ Completed: hais_2024-04-04.csv | Total Rows: 1261808 | Kept: 7046\n",
      "🔄 Re-processing: hais_2024-04-05.csv...\n",
      "✔ Completed: hais_2024-04-05.csv | Total Rows: 1337500 | Kept: 9323\n",
      "🔄 Re-processing: hais_2024-04-26.csv...\n",
      "✔ Completed: hais_2024-04-26.csv | Total Rows: 1196245 | Kept: 6352\n",
      "🔄 Re-processing: hais_2024-04-30.csv...\n",
      "✔ Completed: hais_2024-04-30.csv | Total Rows: 1358923 | Kept: 8709\n",
      "\n",
      "✅ All remaining problematic files processed! Check the 'remaining_processed_oslo_files' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import csv\n",
    "\n",
    "# Define the working folder (where raw CSV files are located)\n",
    "raw_data_folder = os.getcwd()  # Assumes script is in the same folder as raw data\n",
    "\n",
    "# Create a separate folder for remaining processed Oslo files\n",
    "remaining_oslo_folder = os.path.join(raw_data_folder, \"remaining_processed_oslo_files\")\n",
    "os.makedirs(remaining_oslo_folder, exist_ok=True)\n",
    "\n",
    "# Define Oslo Port polygon\n",
    "oslo_polygon = Polygon([\n",
    "    (10.6624659, 59.8957689), (10.7074412, 59.8788027), (10.7706126, 59.898524), \n",
    "    (10.7548197, 59.9150499), (10.6986865, 59.9118659), (10.6624659, 59.8957689)\n",
    "])\n",
    "\n",
    "# List of problematic files to process\n",
    "problematic_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "# Columns to keep for filtering\n",
    "use_columns = [\"date_time_utc\", \"mmsi\", \"longitude\", \"latitude\", \"status\", \n",
    "               \"course_over_ground\", \"speed_over_ground\", \"rate_of_turn\", \n",
    "               \"maneuvre\", \"imo\", \"callsign\", \"ship_name\", \"ship_type\", \n",
    "               \"length\", \"draught\"]\n",
    "\n",
    "# Processing each problematic file\n",
    "for file in problematic_files:\n",
    "    file_path = os.path.join(raw_data_folder, file)\n",
    "    output_path = os.path.join(remaining_oslo_folder, file.replace(\".csv\", \"_oslo_filtered.csv\"))\n",
    "\n",
    "    if os.path.exists(file_path):  # Ensure file exists\n",
    "        print(f\"🔄 Re-processing: {file}...\")\n",
    "\n",
    "        try:\n",
    "            total_rows = 0\n",
    "            filtered_rows = 0\n",
    "\n",
    "            # Process in chunks to avoid memory issues\n",
    "            chunk_size = 100000  \n",
    "            chunks = pd.read_csv(\n",
    "                file_path, \n",
    "                usecols=use_columns, \n",
    "                chunksize=chunk_size, \n",
    "                delimiter=\",\",  # Explicit separator\n",
    "                dtype={\"longitude\": float, \"latitude\": float},  # Force correct data types\n",
    "                on_bad_lines=\"warn\",  # Ignore and warn about bad lines\n",
    "                encoding_errors=\"ignore\",  # Ignore encoding errors\n",
    "                quoting=csv.QUOTE_NONE  # Treat quotes as regular characters\n",
    "            )\n",
    "\n",
    "            for chunk in chunks:\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                # Drop NaN values in critical columns\n",
    "                chunk = chunk.dropna(subset=[\"longitude\", \"latitude\"])\n",
    "\n",
    "                # Convert coordinates to geometry\n",
    "                chunk[\"geometry\"] = [Point(xy) for xy in zip(chunk.longitude, chunk.latitude)]\n",
    "                gdf = gpd.GeoDataFrame(chunk, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "                # Filter rows that fall within the Oslo Port polygon\n",
    "                filtered_gdf = gdf[gdf.geometry.within(oslo_polygon)]\n",
    "\n",
    "                if not filtered_gdf.empty:\n",
    "                    filtered_rows += len(filtered_gdf)\n",
    "\n",
    "                    # Save filtered data incrementally\n",
    "                    filtered_gdf.drop(columns=[\"geometry\"]).to_csv(output_path, mode=\"a\", index=False)\n",
    "\n",
    "            print(f\"✔ Completed: {file} | Total Rows: {total_rows} | Kept: {filtered_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "print(\"\\n✅ All remaining problematic files processed! Check the 'remaining_processed_oslo_files' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb2f25-8391-43bf-95e0-e2f7e2d55513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
